{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82ca1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import open_clip\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925cd22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"BAAI/bge-base-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings1 = outputs.last_hidden_state\n",
    "\n",
    "embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28951f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08c5f70b3be484db85b1c90f9b0c99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81508b01e8b64fdda7e6e66dddd1cedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f8a8ece65a4200b8bcb0c8457491e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4e71a030ba4ec08d93ca7e83975c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78349cca055747ab984790abf5baa087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aef71e500d45a3964d99ac58c954ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings4 = outputs.last_hidden_state\n",
    "\n",
    "embeddings4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb42937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"nomic-ai/nomic-embed-text-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName, trust_remote_code=True)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings2 = outputs.last_hidden_state\n",
    "\n",
    "embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55dae8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092e98fdaa0e42a6b6ce9948ddd716fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 4096])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"Qwen/Qwen3-Embedding-8B\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings3 = outputs.last_hidden_state\n",
    "\n",
    "embeddings3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a507042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-g-14\",\n",
    "    pretrained=\"laion2b_s34b_b88k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-g-14\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(texts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.token_embedding(tokens) \n",
    "    x = x + model.positional_embedding\n",
    "    x = x.permute(1, 0, 2)              \n",
    "\n",
    "    for blk in model.transformer.resblocks:\n",
    "        x = blk(x)\n",
    "\n",
    "    x = x.permute(1, 0, 2)              \n",
    "    embeddings5 = model.ln_final(x)\n",
    "embeddings5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafa5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-xxl\")\n",
    "# t5Encoder = T5EncoderModel.from_pretrained(\n",
    "#     \"google/t5-v1_1-xxl\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# t5Encoder.eval()\n",
    "# inputs = tokenizer(\n",
    "#     texts,\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     # max_length=max_length,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(t5Encoder.device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = t5Encoder(**inputs)\n",
    "#     embedding6 = outputs.last_hidden_state\n",
    "\n",
    "# embedding6.shape\n",
    "\n",
    "## TOO Large File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fec9065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
