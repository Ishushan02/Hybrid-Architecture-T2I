{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ca1a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import open_clip\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925cd22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"BAAI/bge-base-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings1 = outputs.last_hidden_state\n",
    "\n",
    "embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28951f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 1024])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"BAAI/bge-large-en-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors='pt', max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings4 = outputs.last_hidden_state\n",
    "\n",
    "embeddings4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb42937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272ad7e5c446497184b406ad4fe9232c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5f2eb7f4d4f33966eb9632b04fdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfc7cc7917e408b9fb5b063ea8c9659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b20e0eed3ec403e99cbce4717c1eebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6647d13fa2ee4c1193c4ca7964e0cc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d67a32e2b4a62b693684f0a6f5888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/547M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"nomic-ai/nomic-embed-text-v1.5\"#\"nomic-ai/nomic-embed-text-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName, trust_remote_code=True)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings2 = outputs.last_hidden_state\n",
    "\n",
    "embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55dae8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de94c5a349094d31b4123e321a5d967b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"Qwen/Qwen3-Embedding-8B\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModel.from_pretrained(modelName)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings3 = outputs.last_hidden_state\n",
    "\n",
    "embeddings3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d85a7c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 4096])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(texts, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings3 = outputs.last_hidden_state\n",
    "\n",
    "embeddings3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a507042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=\"ViT-g-14\",\n",
    "    pretrained=\"laion2b_s34b_b88k\"\n",
    ")\n",
    "\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-g-14\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(texts).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.token_embedding(tokens) \n",
    "    x = x + model.positional_embedding\n",
    "    x = x.permute(1, 0, 2)              \n",
    "\n",
    "    for blk in model.transformer.resblocks:\n",
    "        x = blk(x)\n",
    "\n",
    "    x = x.permute(1, 0, 2)              \n",
    "    embeddings5 = model.ln_final(x)\n",
    "embeddings5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafa5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = T5Tokenizer.from_pretrained(\"google/t5-v1_1-xxl\")\n",
    "# t5Encoder = T5EncoderModel.from_pretrained(\n",
    "#     \"google/t5-v1_1-xxl\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# t5Encoder.eval()\n",
    "# inputs = tokenizer(\n",
    "#     texts,\n",
    "#     padding=\"max_length\",\n",
    "#     truncation=True,\n",
    "#     # max_length=max_length,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(t5Encoder.device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = t5Encoder(**inputs)\n",
    "#     embedding6 = outputs.last_hidden_state\n",
    "\n",
    "# embedding6.shape\n",
    "\n",
    "## TOO Large File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fec9065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99155a6aa7640e2a88d081956239eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885dd978fe8c49e3a3b2aa511c95cb88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78edc033b71948a2b754aefc694155eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2613893af7480087dc5f68464443ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7738941a59184c1c9345291d80c80067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5aa97302964b41a8e38709098a0075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37f677ea8a74b21a700a3e54783b01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 77, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\"\n",
    ")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    \"openai/clip-vit-large-patch14\"\n",
    ")\n",
    "\n",
    "captions = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    captions,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=77,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(**inputs).last_hidden_state\n",
    "\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c75ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
