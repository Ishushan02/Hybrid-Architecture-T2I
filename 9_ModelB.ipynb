{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75309a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from diffusers import AutoencoderDC\n",
    "import gc\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "import kornia\n",
    "from torchvision.models import vgg16\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccc09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"HYBRID-T2I\",  \n",
    "#     name=\"transformer-Decoder\",    \n",
    "#     # id=\"zgixhfwe\",  \n",
    "#     # resume=\"allow\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61641b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + self.eps))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        # normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device, dtype=x.dtype))\n",
    "\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dea9357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(hiddenDim),\n",
    "            ResidualBlock(hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 4 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(4 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(4 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(embedDim, 4 * hiddenDim, 3, padding=1),\n",
    "            nn.BatchNorm2d(4 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "        \n",
    "            nn.ConvTranspose2d(\n",
    "                4 * hiddenDim, 2 * hiddenDim,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "        \n",
    "            nn.ConvTranspose2d(\n",
    "                2 * hiddenDim, hiddenDim,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "            ResidualBlock(hiddenDim),\n",
    "            ResidualBlock(hiddenDim),\n",
    "        \n",
    "            nn.ConvTranspose2d(\n",
    "                hiddenDim, hiddenDim,\n",
    "                kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        \n",
    "            nn.Conv2d(hiddenDim, inChannels, kernel_size=3, padding=1),\n",
    "            # nn.Sigmoid()\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def encoderBlock(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decoderBlock(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, inChannels, height, width = x.shape\n",
    "        encodedOut = self.encoderBlock(x)\n",
    "        batch_size, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'b c h w -> (b h w) c')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(b h w) d -> b d h w', d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decoderBlock(decoder_input)\n",
    "\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf508128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rotFrequency(headDim, seqLen, theta=10000.0):\n",
    "    assert headDim % 2 == 0\n",
    "    inv_freq = 1.0 / (\n",
    "        theta ** (torch.arange(0, headDim, 2).float() / headDim)\n",
    "    )\n",
    "    positions = torch.arange(seqLen).float()  \n",
    "    freqs = torch.outer(positions, inv_freq)\n",
    "\n",
    "    return freqs\n",
    "\n",
    "freq = rotFrequency(512, 4096)\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19ffaacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096, 16, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ropEQK(x, freqs):\n",
    "    cos = freqs.cos()[None, :, None, :]\n",
    "    sin = freqs.sin()[None, :, None, :]\n",
    "\n",
    "    evenVals = x[..., 0::2]\n",
    "    oddVals  = x[..., 1::2]\n",
    "\n",
    "    rotated = torch.cat(\n",
    "        [\n",
    "            evenVals * cos - oddVals * sin,\n",
    "            evenVals * sin + oddVals * cos\n",
    "        ],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    return rotated\n",
    "\n",
    "q = torch.randn(1, 4096, 16, 512)\n",
    "qRot = ropEQK(q, freq)\n",
    "qRot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d156fcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4096, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dimension):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-6\n",
    "        self.weight = nn.Parameter(torch.ones(dimension))\n",
    "\n",
    "    def forward(self, x):\n",
    "        xNorm = x * torch.rsqrt(x.pow(2).mean(-1, keepdim = True) + self.eps)\n",
    "        normalized = self.weight * xNorm \n",
    "        return normalized.type_as(x)\n",
    "\n",
    "rmsN = RMSNorm(512)\n",
    "embeddings = torch.randn(2, 4096, 512)\n",
    "norm = rmsN(embeddings)\n",
    "norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21a2adc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024, 4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, dimension=2048, latentDim = 4096, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dimension\n",
    "        \n",
    "        self.w1 = nn.Linear(dimension, hidden_dim)  \n",
    "        self.w2 = nn.Linear(hidden_dim, latentDim)  \n",
    "        self.w3 = nn.Linear(dimension, hidden_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        swish = Fn.silu(self.w1(x))  \n",
    "        xV = self.w3(x)\n",
    "        x = swish * xV\n",
    "        \n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "ffn = FeedForwardLayer(dimension=2048)\n",
    "x = torch.randn(2, 1024, 2048)\n",
    "out = ffn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7ca4ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 2048])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadSelfAttentionGating(nn.Module):\n",
    "    def __init__(self, embedDimension, numHeads, dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embedDimension%numHeads == 0, \"Embedding Dimension is Not Divisible By NumHeads\"\n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "        self.headDim = embedDimension//numHeads\n",
    "\n",
    "        self.queryKeyValue = nn.Linear(embedDimension, embedDimension * 3, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.gateProj = nn.Linear(embedDimension, embedDimension, bias = False)\n",
    "        self.scale = self.headDim ** -0.5 \n",
    "        self.outProjection = nn.Linear(embedDimension, embedDimension)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.queryKeyValue.weight)\n",
    "        nn.init.xavier_uniform_(self.outProjection.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        BatchSize, seqLen, EmbedDim = x.shape\n",
    "\n",
    "        qkv = self.queryKeyValue(x)\n",
    "        qkv = qkv.reshape(BatchSize, seqLen, 3, self.numHeads, EmbedDim // self.numHeads)\n",
    "        # rotFrequency(512, 4096)\n",
    "\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        frequencies = rotFrequency(self.headDim, seqLen)\n",
    "        qRot = ropEQK(q, frequencies)\n",
    "        kRot = ropEQK(k, frequencies)\n",
    "\n",
    "        q = qRot.transpose(1, 2)\n",
    "        k = kRot.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "\n",
    "        attentionScore = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        att = attentionScore.softmax(dim=-1)\n",
    "        out = att @ v \n",
    "\n",
    "        # print(out.shape)\n",
    "        gate = torch.sigmoid(self.gateProj(x))\n",
    "        gate = gate.reshape(BatchSize, seqLen, self.numHeads, self.headDim).transpose(1, 2)\n",
    "        # print(gate.shape, out.shape)\n",
    "\n",
    "        out = out * gate\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(BatchSize, seqLen, EmbedDim)\n",
    "        out = self.outProjection(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "    \n",
    "mhsa = MultiHeadSelfAttentionGating(embedDimension = 2048, numHeads = 16)\n",
    "x = torch.randn(2, 64, 2048)\n",
    "out = mhsa(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d843427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 2048])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextToImageLatentModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedDimension = 2048, textEmbed = 768,  numHeads = 16, latentDim = 2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "        self.latentDim = latentDim\n",
    "\n",
    "        self.mhsa = MultiHeadSelfAttentionGating(self.embedDimension, self.numHeads)\n",
    "        \n",
    "        self.rmsNorm = RMSNorm(self.embedDimension)\n",
    "\n",
    "        self.feedForward = FeedForwardLayer(dimension = embedDimension, latentDim =  latentDim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchSize, seqlen, _ = x.shape\n",
    "\n",
    "        x1 = x\n",
    "        x = self.rmsNorm(x1)\n",
    "\n",
    "        x = self.mhsa(x)\n",
    "        \n",
    "        x1 = x1 + x\n",
    "\n",
    "        x = self.rmsNorm(x1)\n",
    "        x = self.feedForward(x)\n",
    "        # print(x.shape, x1.shape)\n",
    "        x = x1 + x\n",
    "\n",
    "        return x\n",
    "\n",
    "t2iM = TextToImageLatentModel()\n",
    "x = torch.randn(2, 512, 2048)\n",
    "out = t2iM(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "082d0916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4096])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NLayerT2I(nn.Module):\n",
    "\n",
    "    def __init__(self, embedDimension = 2048, textEmbed = 768,  numHeads = 16, outDimension = 4096, nBlocks = 6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedDimension = embedDimension\n",
    "        self.numHeads = numHeads\n",
    "        self.textEmbed = textEmbed\n",
    "\n",
    "        self.linearTextProjection = nn.Linear(textEmbed, embedDimension)\n",
    "\n",
    "        TextToImageLatentModel(embedDimension = self.embedDimension, textEmbed = self.textEmbed,  numHeads = self.numHeads, latentDim = 2048)\n",
    "\n",
    "        self.nAttentionBlocks = nn.ModuleList([\n",
    "            TextToImageLatentModel(embedDimension = self.embedDimension, textEmbed = self.textEmbed,  numHeads = self.numHeads, latentDim = 2048)\n",
    "            for _ in range(nBlocks)\n",
    "        ])\n",
    "\n",
    "        self.rmsNorm2 = RMSNorm(self.embedDimension)\n",
    "        self.outputLayer = nn.Linear(embedDimension, outDimension)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batchSize, seqlen, _ = x.shape\n",
    "\n",
    "        x = self.linearTextProjection(x)\n",
    "\n",
    "        for block in self.nAttentionBlocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.rmsNorm2(x)\n",
    "        x = x.mean(1)\n",
    "        x = self.outputLayer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "finalModel = NLayerT2I()\n",
    "x = torch.randn(2, 512, 768)\n",
    "out = finalModel(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3853f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"nomic-ai/nomic-embed-text-v1.5\" #\"nomic-ai/nomic-embed-text-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "TEXTMODEL = AutoModel.from_pretrained(modelName, trust_remote_code=True)\n",
    "\n",
    "texts = [\n",
    "    \"To dos walking towards each other\",\n",
    "    \"All Dogs are playing in the Garden\",\n",
    "    \"Generate Image of Dog\"\n",
    "]\n",
    "\n",
    "textInputs = tokenizer(texts, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = textModel(**textInputs)\n",
    "    embeddings2 = outputs.last_hidden_state\n",
    "\n",
    "embeddings2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb5a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/Desktop/Hybrid-Architecture-T2I/models/vqvae/vqvae-v2.pt\n"
     ]
    }
   ],
   "source": [
    "CODEBOOKDIM = 1024\n",
    "EMBEDDIM = 128\n",
    "HIDDENDIM = 256\n",
    "INPCHANNELS = 3\n",
    "modelA = VecQVAE(inChannels = INPCHANNELS, hiddenDim = HIDDENDIM, codeBookdim = CODEBOOKDIM, embedDim = EMBEDDIM).to(device)\n",
    "modelA = torch.nn.DataParallel(modelA)\n",
    "modelA = modelA.to(device)\n",
    "optimizerA = torch.optim.Adam([\n",
    "                    {'params': modelA.module.encoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.module.decoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.module.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ])#, weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "            \n",
    "start_epoch = 0\n",
    "baseDir = os.getcwd()#os.path.dirname(__file__)\n",
    "# baseDir = os.path.dirname(__file__)\n",
    "checkpoint_path = os.path.join(baseDir, \"models/vqvae\", \"vqvae-v2.pt\")\n",
    "print(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_state_dict['module.' + k] = v  # add 'module.' prefix\n",
    "    modelA.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # modelA.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizerA.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    schedulerA.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    lowestVQVAELoss = checkpoint['lowestLoss']\n",
    "    # lowestVQVAELoss = 10.9482\n",
    "    for state in optimizerA.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "    # print(f\"Resuming from epoch {start_epoch} and lowest loss is {lowestVQVAELoss}\")\n",
    "else:\n",
    "    lowestVQVAELoss = float('inf')\n",
    "    # print(\"Loading pretrained model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "57a1a661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 768]), torch.Size([4096]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, data, modelA, imageDim, rootDir = \"\"):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.rootDir = rootDir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((imageDim, imageDim)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "        self.modelAVQVAE = modelA\n",
    "        # self.modelAVQVAE = self.modelAVQVAE.to(device)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "        image_path = os.path.join(self.rootDir, row['imagePath'])\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        captions = [\n",
    "            row['caption1'],\n",
    "            row['caption2'],\n",
    "            row['caption3'],\n",
    "            row['caption4'],\n",
    "            row['caption5']\n",
    "        ]\n",
    "\n",
    "        caption = random.choice(captions)\n",
    "\n",
    "        textInputs = tokenizer(caption, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = TEXTMODEL(**textInputs)\n",
    "            textEmbeddings = outputs.last_hidden_state\n",
    "\n",
    "        self.modelAVQVAE.eval()\n",
    "        with torch.no_grad():\n",
    "            if(len(image.shape) == 3):\n",
    "                image = image.unsqueeze(0)\n",
    "            _, _, _, _, encoding_indices, _, _ = modelA(image)\n",
    "    \n",
    "        \n",
    "        return textEmbeddings, encoding_indices\n",
    "\n",
    "# datasetPath = \"Tiny-Recursive-Model-for-Text-To-Image-Generation/\"\n",
    "datasetPath = \"/Users/ishananand/Desktop/Tiny-Recursive-Model-for-Text-To-Image-Generation/\"\n",
    "data = pd.read_csv(datasetPath + \"dataset/COCO2017.csv\")\n",
    "\n",
    "imgData = ImageTextDataset(data, modelA, 512, rootDir = datasetPath)\n",
    "\n",
    "x, y = imgData.__getitem__(0)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3496205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 865878016\n"
     ]
    }
   ],
   "source": [
    "IMAGEDIMENSION = 512\n",
    "EMBEDDINGDIM = 2048\n",
    "BATCHSIZE = 2\n",
    "INCHANNELS = 3\n",
    "LATENTSIZE = 8\n",
    "LATENTCHANNEL = 128\n",
    "PATCHSIZE = 2\n",
    "NBLOCKS = 12\n",
    "NHEADS = 16\n",
    "dropout = 0.2\n",
    "CODEBOOKDIM = 1024\n",
    "TEXTEMBED = 768\n",
    "OUTDIMENSION = 4096\n",
    "\n",
    "EPCOHS = 100000\n",
    "\n",
    "# datasetPath = \"Tiny-Recursive-Model-for-Text-To-Image-Generation/\"\n",
    "datasetPath = \"/Users/ishananand/Desktop/Tiny-Recursive-Model-for-Text-To-Image-Generation/\"\n",
    "data = pd.read_csv(datasetPath + \"dataset/COCO2017.csv\")\n",
    "\n",
    "torchDataset = ImageTextDataset(data, modelA, IMAGEDIMENSION, rootDir = datasetPath)\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCHSIZE, shuffle = True, num_workers=8, pin_memory=True,persistent_workers=True)\n",
    "\n",
    "\n",
    "\n",
    "modelB = NLayerT2I(embedDimension = EMBEDDINGDIM, textEmbed = TEXTEMBED,  numHeads = NHEADS, outDimension = OUTDIMENSION, nBlocks = NBLOCKS)\n",
    "modelB = modelB.to(device)\n",
    "print(f\"Total Parameters: {sum(p.numel() for p in modelB.parameters() if p.requires_grad)}\")\n",
    "\n",
    "stepsPerEpochs = len(dataloader)\n",
    "lossFn =  nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(params=modelB.parameters(), lr=5e-5, weight_decay=1e-2)#2e-5)#, weight_decay=3e-2, eps=1e-10)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer, T_0= 5 * stepsPerEpochs, T_mult=2, eta_min=1e-6\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a95c2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/Desktop/Hybrid-Architecture-T2I/models/transformer/decoder.pt\n",
      "Loading pretrained model...\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "baseDir = os.getcwd()#os.path.dirname(__file__)\n",
    "# baseDir = os.path.dirname(__file__)\n",
    "checkpoint_path = os.path.join(baseDir, \"models/transformer\", \"decoder.pt\")\n",
    "print(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    new_state_dict = {}\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_state_dict['module.' + k] = v  # add 'module.' prefix\n",
    "    modelB.load_state_dict(new_state_dict)\n",
    "    \n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    lowestDecoderLoss = checkpoint['lowestLoss']\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "    print(f\"Resuming from epoch {start_epoch} and lowest loss is {lowestDecoderLoss}\")\n",
    "else:\n",
    "    lowestDecoderLoss = float('inf')\n",
    "    print(\"Loading pretrained model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8c21802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y8/q2s37ndx6tg3lpzp3vp8xk_r0000gn/T/ipykernel_18551/3876566364.py:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "0/100000:   0%|          | 0/61644 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=79, pipe_handle=116)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'ImageTextDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n",
      "0/100000:   0%|          | 0/61644 [3:02:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m latentLoss = \u001b[32m0.0\u001b[39m\n\u001b[32m      8\u001b[39m loop = tqdm(dataloader, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meach_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPCOHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:486\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persistent_workers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers > \u001b[32m0\u001b[39m:\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    488\u001b[39m         \u001b[38;5;28mself\u001b[39m._iterator._reset(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:422\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1146\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1139\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1142\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1144\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1145\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1146\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1148\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/context.py:289\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "scaler = GradScaler()\n",
    "\n",
    "for each_epoch in range(start_epoch, EPCOHS):\n",
    "    modelB.train()\n",
    "    latentLoss = 0.0\n",
    "    \n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{EPCOHS}\")\n",
    "\n",
    "    for (X,Y) in loop:\n",
    "        \n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        with autocast():\n",
    "            output = modelA(X)\n",
    "            lossVal = loss(decoderOut, X)\n",
    "        \n",
    "        latentLoss += lossVal.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(lossVal).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(modelB.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        loop.set_postfix({\n",
    "            \"Loss \": f\"{lossVal.item():.6f}\",\n",
    "            })\n",
    "        # break\n",
    "    # break\n",
    "\n",
    "    latentLoss /= len(dataloader)   \n",
    "\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    if(latentLoss < lowestDecoderLoss):\n",
    "        print(\"Lowest Loss up until now is: \", latentLoss)\n",
    "        lowestDecoderLoss = latentLoss\n",
    "        baseDir = os.path.dirname(__file__)\n",
    "        lowestLossPath = os.path.join(baseDir, \"models/lowestLoss\", \"decoder.pt\")\n",
    "        os.makedirs(os.path.dirname(lowestLossPath), exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': each_epoch,\n",
    "            'model_state_dict': model.module.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'lowestLoss': latentLoss\n",
    "        }, lowestLossPath)\n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': each_epoch,\n",
    "        'model_state_dict': model.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'lowestLoss': latentLoss\n",
    "    }, lowestLossPath)\n",
    "\n",
    "    # wandb.log({\n",
    "    #     \"Epoch\": each_epoch,\n",
    "    #     \"Decoder LR\": optimizer.param_groups[0]['lr'],\n",
    "    #     \"Decoder Loss\": vqvaeloss,\n",
    "    # })\n",
    "    scheduler.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5cc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
