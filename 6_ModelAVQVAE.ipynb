{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ea31914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchview import draw_graph\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional  as Fn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from piq import ssim\n",
    "import pandas as pd\n",
    "from diffusers import AutoencoderDC\n",
    "import gc\n",
    "from torchvision import transforms\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7c3cac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6526318",
   "metadata": {},
   "source": [
    "## Most of My this Part of Code are from my earlier Implementation\n",
    "https://github.com/Ishushan02/Video-Generation-Flowing-MNIST/blob/main/model-4-VQVAE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f2a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1088, -0.0109, -0.1961,  ..., -0.1223,  0.1196,  0.1098],\n",
       "         [-0.1562, -0.2180,  0.1722,  ...,  0.0227, -0.2160,  0.0699],\n",
       "         [ 0.2184, -0.2055,  0.1569,  ..., -0.1674, -0.2436,  0.0372],\n",
       "         ...,\n",
       "         [-0.2120,  0.1694, -0.1444,  ..., -0.0138, -0.1555,  0.2340],\n",
       "         [ 0.1940, -0.0664, -0.0706,  ..., -0.0501,  0.0444,  0.1781],\n",
       "         [-0.1320,  0.2088,  0.1111,  ..., -0.2188,  0.2006, -0.0126]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([57, 39, 18,  ..., 61, 24, 46]),\n",
       " tensor(60.0664),\n",
       " tensor(0.0153))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + 1e-10))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss\n",
    "        \n",
    "        \n",
    "vq = VectorQuantizeImage(codeBookDim=64,embeddingDim=32)\n",
    "rand = torch.randn(1024,32)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88990386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1024, 1, 1]),\n",
       " torch.Size([2, 128, 8, 8]),\n",
       " tensor(0.6453, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.6453, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([2]),\n",
       " tensor(2.),\n",
       " tensor(0.9000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(hiddenDim),\n",
    "            ResidualBlock(hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 4 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(4 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            \n",
    "            nn.Conv2d(4 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(embedDim, 4 * hiddenDim, 1),\n",
    "            nn.BatchNorm2d(4 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            ResidualBlock(4 * hiddenDim),\n",
    "            nn.ConvTranspose2d(4 * hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            ResidualBlock(2 * hiddenDim),\n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            ResidualBlock(hiddenDim),\n",
    "            ResidualBlock(hiddenDim),\n",
    "            nn.ConvTranspose2d(hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def encoderBlock(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decoderBlock(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, inChannels, height, width = x.shape\n",
    "        encodedOut = self.encoderBlock(x)\n",
    "        batch_size, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'b c h w -> (b h w) c')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(b h w) d -> b d h w', d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decoderBlock(decoder_input)\n",
    "\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 128, hiddenDim = 256, codeBookdim = 1024, embedDim = 1024)\n",
    "test = torch.randn(2, 128, 8, 8)\n",
    "\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21efae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCAEEncoder = AutoencoderDC.from_pretrained(f\"mit-han-lab/dc-ae-f64c128-in-1.0-diffusers\", torch_dtype=torch.float32).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8b7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb36565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b5347f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imagePath</th>\n",
       "      <th>caption1</th>\n",
       "      <th>caption2</th>\n",
       "      <th>caption3</th>\n",
       "      <th>caption4</th>\n",
       "      <th>caption5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/train2017/000000203564.jpg</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "      <td>The bike has a clock as a tire.</td>\n",
       "      <td>A black metal bicycle with a clock inside the ...</td>\n",
       "      <td>A bicycle figurine in which the front wheel is...</td>\n",
       "      <td>A clock with the appearance of the wheel of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/train2017/000000322141.jpg</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "      <td>Blue and white color scheme in a small bathroom.</td>\n",
       "      <td>This is a blue and white bathroom with a wall ...</td>\n",
       "      <td>A blue boat themed bathroom with a life preser...</td>\n",
       "      <td>A bathroom with walls that are painted baby blue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/train2017/000000016977.jpg</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "      <td>two cars parked on the sidewalk on the street</td>\n",
       "      <td>City street with parked cars and a bench.</td>\n",
       "      <td>Cars try to maneuver into parking spaces along...</td>\n",
       "      <td>A couple of cars parked in a busy street sidew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/train2017/000000106140.jpg</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "      <td>There is a GOL plane taking off in a partly cl...</td>\n",
       "      <td>An airplane that is, either, landing or just t...</td>\n",
       "      <td>An red and white airplane is in the cloudy sky.</td>\n",
       "      <td>A passenger plane taking off into the sky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/train2017/000000571635.jpg</td>\n",
       "      <td>A bathroom with a toilet, sink, and shower.</td>\n",
       "      <td>A full bathroom with a wicker laundry basket.</td>\n",
       "      <td>A little bathrood decorated with many colorful...</td>\n",
       "      <td>A small bathroom containing a toilet and sink.</td>\n",
       "      <td>Bathroom containing a toilet, a sink and a wic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            imagePath  \\\n",
       "0  dataset/train2017/000000203564.jpg   \n",
       "1  dataset/train2017/000000322141.jpg   \n",
       "2  dataset/train2017/000000016977.jpg   \n",
       "3  dataset/train2017/000000106140.jpg   \n",
       "4  dataset/train2017/000000571635.jpg   \n",
       "\n",
       "                                            caption1  \\\n",
       "0  A bicycle replica with a clock as the front wh...   \n",
       "1  A room with blue walls and a white sink and door.   \n",
       "2  A car that seems to be parked illegally behind...   \n",
       "3  A large passenger airplane flying through the ...   \n",
       "4        A bathroom with a toilet, sink, and shower.   \n",
       "\n",
       "                                            caption2  \\\n",
       "0                    The bike has a clock as a tire.   \n",
       "1   Blue and white color scheme in a small bathroom.   \n",
       "2      two cars parked on the sidewalk on the street   \n",
       "3  There is a GOL plane taking off in a partly cl...   \n",
       "4      A full bathroom with a wicker laundry basket.   \n",
       "\n",
       "                                            caption3  \\\n",
       "0  A black metal bicycle with a clock inside the ...   \n",
       "1  This is a blue and white bathroom with a wall ...   \n",
       "2          City street with parked cars and a bench.   \n",
       "3  An airplane that is, either, landing or just t...   \n",
       "4  A little bathrood decorated with many colorful...   \n",
       "\n",
       "                                            caption4  \\\n",
       "0  A bicycle figurine in which the front wheel is...   \n",
       "1  A blue boat themed bathroom with a life preser...   \n",
       "2  Cars try to maneuver into parking spaces along...   \n",
       "3    An red and white airplane is in the cloudy sky.   \n",
       "4    A small bathroom containing a toilet and sink.    \n",
       "\n",
       "                                            caption5  \n",
       "0  A clock with the appearance of the wheel of a ...  \n",
       "1  A bathroom with walls that are painted baby blue.  \n",
       "2  A couple of cars parked in a busy street sidew...  \n",
       "3        A passenger plane taking off into the sky.   \n",
       "4  Bathroom containing a toilet, a sink and a wic...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetPath = \"/Users/ishananand/Desktop/Tiny-Recursive-Model-for-Text-To-Image-Generation/\"\n",
    "data = pd.read_csv(datasetPath + \"dataset/COCO2017.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f329697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, rootDir = \"\"):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.rootDir = rootDir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512, 512)),\n",
    "            transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "\n",
    "        image_path = os.path.join(self.rootDir, row['imagePath'])\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        # with torch.no_grad():\n",
    "        #     latents = DCAEEncoder.encode(image).latent\n",
    "        return image\n",
    "\n",
    "imgtxtdata = ImageDataset(data, rootDir=datasetPath)\n",
    "\n",
    "img = imgtxtdata.__getitem__(0)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93672340",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 16\n",
    "CODEBOOKDIM = 1024\n",
    "EMBEDDIM = 1025\n",
    "HIDDENDIM = 256\n",
    "INPCHANNELS = 128\n",
    "torchDataset = ImageDataset(data, rootDir=datasetPath)\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCHSIZE, shuffle = True)\n",
    "modelA = VecQVAE(inChannels = INPCHANNELS, hiddenDim = HIDDENDIM, codeBookdim = CODEBOOKDIM, embedDim = EMBEDDIM).to(device)\n",
    "# lossFn = nn.MSELoss()\n",
    "optimizerA = torch.optim.Adam([\n",
    "                    {'params': modelA.encoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.decoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ], weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb953f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/Desktop/Hybrid-Architecture-T2I/models/vqvae/vqvae.pt\n",
      "Loading pretrained model...\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "baseDir = os.getcwd()#os.path.dirname(__file__)\n",
    "checkpoint_path = os.path.join(baseDir, \"models/vqvae\", \"vqvae.pt\")\n",
    "print(checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    modelA.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizerA.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    schedulerA.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    for state in optimizerA.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.to(device)\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Loading pretrained model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 1/7706 [12:53<1654:45:04, 773.15s/it, TotalL=3.3673195838928223, ReconsL=2.5073745250701904, CodeL=0.6634957194328308, CommitL=0.6634957194328308, Perplexity=12.337687492370605, Diversity Loss=0.637499988079071]"
     ]
    }
   ],
   "source": [
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelA.train()\n",
    "    reconstruct_loss = 0.0\n",
    "    codeb_loss = 0.0\n",
    "    commit_loss = 0.0\n",
    "    vqvaeloss = 0.0\n",
    "    diverse_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "    perplexities = []\n",
    "\n",
    "    for images in loop:\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X = DCAEEncoder.encode(images).latent\n",
    "        \n",
    "        X = X.to(device)\n",
    "        Y = X\n",
    "        # print(X.shape, Y.shape)\n",
    "    #     break\n",
    "    # break\n",
    "        \n",
    "        quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(X)\n",
    "\n",
    "        # ssim_score = ssim(Y, torch.clamp(decoderOut, 0.0, 1.0), data_range=1.0)\n",
    "        # ssim_score = ssim(Y, decoderOut, data_range=1.0)\n",
    "        # ssim_loss = 1.0 - ssim_score\n",
    "\n",
    "        # reconstruction_loss = torch.mean((Y - decoderOut)**2)\n",
    "        reconstruction_loss = torch.mean(torch.abs(Y - decoderOut))\n",
    "        \n",
    "        loss = reconstruction_loss + codebook_loss + 0.2 * commitment_loss + 0.1 * diversity_loss #+ 0.1 * ssim_loss\n",
    "        vqvaeloss += loss.item()\n",
    "\n",
    "        \n",
    "        reconstruct_loss += reconstruction_loss.item()\n",
    "        diverse_loss += diversity_loss.item()\n",
    "        codeb_loss += codebook_loss.item()\n",
    "        commit_loss += commitment_loss.item()\n",
    "        # ssim_loss += ssim_loss.item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        \n",
    "        optimizerA.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), max_norm=1.0)\n",
    "        optimizerA.step()\n",
    "        loop.set_postfix({\"TotalL\": f\"{vqvaeloss}\", \"ReconsL\": f\"{reconstruct_loss}\", \"CodeL\":f\"{codeb_loss}\",\n",
    "                          \"CommitL\":f\"{commitment_loss}\", \"Perplexity\":f\"{perplexity}\", \"Diversity Loss\":f\"{diverse_loss}\",# \"SSIM Loss\":f\"{ssim_loss}\"\n",
    "                          })\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    average_perplexity = sum(perplexities)/len(perplexities)\n",
    "    vqvaeloss /= len(dataloader)   \n",
    "    reconstruct_loss /= len(dataloader)   \n",
    "    codeb_loss /= len(dataloader)   \n",
    "    commit_loss /= len(dataloader)   \n",
    "    diverse_loss /= len(dataloader)\n",
    "    os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': each_epoch,\n",
    "        'model_state_dict': modelA.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizerA.state_dict(),\n",
    "        'scheduler_state_dict': schedulerA.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    # wandb.log({\n",
    "    #     \"Epoch\": each_epoch,\n",
    "    #     \"VQVAE LR\": optimizerA.param_groups[0]['lr'],\n",
    "    #     \"VQVAE Loss\": vqvaeloss,\n",
    "    #     \"Reconstruction Loss\": reconstruct_loss,\n",
    "    #     \"Codebook Loss\": codeb_loss,\n",
    "    #     \"Commitment Loss\": commit_loss,\n",
    "    #     \"Diversity Loss\": diverse_loss,\n",
    "    #     \"Perplexity\": average_perplexity,\n",
    "    #    # \"SSIM Loss\":ssim_loss,\n",
    "    # })\n",
    "    schedulerA.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379107d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
